{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "383062aa",
      "metadata": {
        "id": "383062aa"
      },
      "source": [
        "# (Fine-Tuning and) Inference with Hugging Face Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5466ac07",
      "metadata": {
        "id": "5466ac07"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate --quiet\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU instead.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5f2a12",
      "metadata": {
        "id": "ea5f2a12"
      },
      "source": [
        "## Step 1: Import Libraries and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5c1c5b",
      "metadata": {
        "id": "5a5c1c5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_from_disk\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dF2XTURICZFE",
      "metadata": {
        "id": "dF2XTURICZFE"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "csv_url = \"./Rohdaten CSV.csv\"                  # CSV file URL or path\n",
        "data = pd.read_csv(csv_url)\n",
        "\n",
        "index_columns = data.columns[1:6]               # Assuming the last column is the Target, 5 features, fewer features so it fits the context window, otherwise too computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vDKyw8whE2x",
      "metadata": {
        "id": "7vDKyw8whE2x"
      },
      "outputs": [],
      "source": [
        "# Splitting examples and test data\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(data)                                  # [['input_text']])\n",
        "\n",
        "# Split into train and validation sets\n",
        "hf_dataset = hf_dataset.train_test_split(test_size=0.1)                 # telling us that model is running 10% of the data, only 10% with taking 5 features into account\n",
        "\n",
        "# store the complete dataset\n",
        "hf_dataset.save_to_disk(\"complete_hf_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52SRQiw-hMIx",
      "metadata": {
        "id": "52SRQiw-hMIx"
      },
      "outputs": [],
      "source": [
        "# TRY RANDOM.SEED\n",
        "\n",
        "def createPromptData(row, target=False):\n",
        "    # Convert the 5 columns (because more is computationally expensive) to strings and join them with a space\n",
        "    line_text = ' '.join(str(row[col]) for col in index_columns)\n",
        "\n",
        "    # Append the target value if specified\n",
        "    if target:\n",
        "        line_text += f\" Target: {int(row['Target'])}\"\n",
        "\n",
        "    return line_text\n",
        "\n",
        "def generateExamples(data, window_size=5):  # 5 examples for the model\n",
        "    # Get random indices for sampling\n",
        "    random_indices = random.sample(range(len(data)), window_size) # VARIATION COMES FROM HERE, SET THE RANDOM SO THERES NO VARIATION, RANDOM.SEED, SET IT AT TOP OF ALL THESE FUNCTIONS\n",
        "\n",
        "    # Retrieve the examples using Hugging Face Dataset indexing\n",
        "    examples = [data[i][\"line_text\"] for i in random_indices]\n",
        "    return examples\n",
        "\n",
        "def generatePrompts(traindata, testdata):\n",
        "    # Add input_text to testdata\n",
        "    testdata = testdata.map(\n",
        "        lambda row: {\n",
        "            \"input_text\": \"\\n\".join(generateExamples(traindata, 5) + [row[\"line_text\"]])\n",
        "        }\n",
        "    )\n",
        "    return testdata  # Ensure the modified dataset is returned\n",
        "\n",
        "print(hf_dataset.column_names)\n",
        "hf_dataset[\"train\"] = hf_dataset[\"train\"].map(lambda row: {\"line_text\": createPromptData(row, True)})\n",
        "hf_dataset[\"test\"] = hf_dataset[\"test\"].map(lambda row: {\"line_text\": createPromptData(row, True)})\n",
        "\n",
        "hf_dataset.save_to_disk(\"final_hf_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99_H_HBIhU-J",
      "metadata": {
        "id": "99_H_HBIhU-J"
      },
      "source": [
        "# Load complete dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4LZnq0EChQTx",
      "metadata": {
        "id": "4LZnq0EChQTx"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL\n",
        "\n",
        "hf_dataset = load_from_disk(\"final_hf_dataset\")\n",
        "hf_dataset.push_to_hub(\"Petar-Uni-Freiburg/LLM_Time_Series\", private=True)\n",
        "print(hf_dataset[\"test\"][0][\"line_text\"]) # test output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df5dfc9",
      "metadata": {
        "id": "8df5dfc9"
      },
      "source": [
        "## Step 2: Load and run the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BaGsFsgDjf6O",
      "metadata": {
        "id": "BaGsFsgDjf6O"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"openlm-research/open_llama_3b_v2\"                                   # Replace with your model name (e.g., \"facebook/llama\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token                                         # Use the EOS token as the padding token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = model.to(device)                                                          # Move the model to the selected device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c88efa6",
      "metadata": {
        "id": "2c88efa6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Inside your `tokenize_function`:\n",
        "def tokenize_function(examples):\n",
        "    tokenized_inputs = tokenizer(examples['input_text'], truncation=True, max_length=512, padding='max_length', return_tensors=\"pt\")\n",
        "    tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}  # Move tokenized inputs to device\n",
        "    return tokenized_inputs\n",
        "\n",
        "hf_dataset[\"test\"] = generatePrompts(hf_dataset[\"train\"], hf_dataset[\"test\"])\n",
        "tokenized_data = hf_dataset#.map(tokenize_function, batched=True)\n",
        "\n",
        "def generatePrediction(row):\n",
        "  # Generate the prompt by removing the actual Target, so the LLM has to predict\n",
        "  input = tokenize_function({\"input_text\": row[\"input_text\"][:-1]})                                 # the -1 eliminates the target column so the model has to make an inference/prediction as to the most likely next token\n",
        "                                                                                                 #print(input)\n",
        "                                                                                                 #print(tokenizer.decode(input[\"input_ids\"][0], skip_special_tokens=True))\n",
        "\n",
        "                                                                                                 #print(input)\n",
        "  output = model.generate(**input, max_new_tokens=1)                                             # here we give the training data as an input, and the model isnt really trained, but its a small scale of training/ completion and it completes the text with the most liekly next token, and when the model sees the trainign data it commits the patterns to the short term working memory, so its not really training, but kind of\n",
        "  truncated_output = output[:, input['input_ids'].shape[1]: input['input_ids'].shape[1] + 1]     # Keep only the first generated token\n",
        "  del input\n",
        "\n",
        "  #gc.collect()\n",
        "  #torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "  # Decode and print the prediction\n",
        "  predicted_target = tokenizer.decode(truncated_output[0], skip_special_tokens=True)\n",
        "  #print(\"Predicted Target:\", predicted_target)\n",
        "  return int(predicted_target) # transforms token to be a 1 digit int output prediction, and we dont have to binary encode bc model saw our training data, and knows to output a  0 or 1\n",
        "\n",
        "#for row in tokenized_data['test']:\n",
        "#  generatePrediction(row)\n",
        "prediction = generatePrediction(tokenized_data['test'][0])\n",
        "\n",
        "# test outputs\n",
        "print(tokenized_data['test'][0][\"input_text\"])\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hJONwSjvhGO1",
      "metadata": {
        "id": "hJONwSjvhGO1"
      },
      "outputs": [],
      "source": [
        "from os import write\n",
        "import sys\n",
        "\n",
        "results = {\n",
        "    \"correct\": 0,\n",
        "    \"false_positive\": 0,\n",
        "    \"false_negative\": 0,\n",
        "    \"total\": 0\n",
        "}\n",
        "\n",
        "for row in tokenized_data['test']:\n",
        "  prediction = generatePrediction(row)\n",
        "  if prediction == int(row[\"line_text\"][-1]):\n",
        "    results[\"correct\"] += 1\n",
        "    sys.stdout.write(\".\")\n",
        "  elif prediction == 1 and row[\"line_text\"][-1] == \"0\":\n",
        "    results[\"false_positive\"] += 1\n",
        "    sys.stdout.write(\"+\")\n",
        "  elif prediction == 0 and row[\"line_text\"][-1] == \"1\":\n",
        "    results[\"false_negative\"] += 1\n",
        "    sys.stdout.write(\"-\")\n",
        "  else:\n",
        "    sys.stdout.write(\"?\")\n",
        "  results[\"total\"] += 1\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N20fPPuqlcyW",
      "metadata": {
        "id": "N20fPPuqlcyW"
      },
      "outputs": [],
      "source": [
        "# not required, only for RAM cleanup\n",
        "del model\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4qUqBAiJqlMw",
      "metadata": {
        "id": "4qUqBAiJqlMw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "del results[\"total\"]\n",
        "labels = list(results.keys())  # Get the keys of the results dictionary as labels\n",
        "sizes = list(results.values())  # Get the values of the results dictionary as sizes\n",
        "\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title('Results Pie Chart')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sRemTGcijMrO",
      "metadata": {
        "id": "sRemTGcijMrO"
      },
      "outputs": [],
      "source": [
        "# as of 16.01, stop here.\n",
        "\n",
        "# TODO fine-tune the model (didnt work so far)\n",
        "hf_dataset[\"test\"] = hf_dataset[\"test\"].remove_columns(\"input_text\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "917a7f25",
      "metadata": {
        "id": "917a7f25"
      },
      "source": [
        "## Step 3: Save the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hj5SiMkJRdeB",
      "metadata": {
        "id": "Hj5SiMkJRdeB"
      },
      "outputs": [],
      "source": [
        "# TODO try and fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fa5844",
      "metadata": {
        "id": "a7fa5844"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer.save_model(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01012d93",
      "metadata": {
        "id": "01012d93"
      },
      "source": [
        "## Step 4: Load and Infer with Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f130aff",
      "metadata": {
        "id": "4f130aff"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fine_tuned_model_path = \"./fine_tuned_model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Prepare input for inference\n",
        "input_text = \"0.01 0.02 0.03 ... 0.05 Target:\"\n",
        "tokenized_input = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate prediction\n",
        "output = model.generate(**tokenized_input, max_new_tokens=1)\n",
        "\n",
        "# Decode and print the prediction\n",
        "predicted_target = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Predicted Target:\", predicted_target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a129bff0",
      "metadata": {
        "id": "a129bff0"
      },
      "source": [
        "## Step 5: Batch Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15ee5a0",
      "metadata": {
        "id": "d15ee5a0"
      },
      "outputs": [],
      "source": [
        "# TODO modifz to work with the complete hf_dataset test data\n",
        "\n",
        "# Batch inference for multiple inputs\n",
        "input_texts = [\"0.01 0.02 0.03 ... 0.05 Target:\", \"0.02 0.03 0.04 ... 0.06 Target:\"]\n",
        "\n",
        "# Tokenize inputs\n",
        "batch_inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Generate predictions\n",
        "outputs = model.generate(**batch_inputs, max_new_tokens=1)\n",
        "\n",
        "# Decode predictions\n",
        "predicted_targets = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "print(\"Predicted Targets:\", predicted_targets)\n",
        "\n",
        "# TODO generate pie chart"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
